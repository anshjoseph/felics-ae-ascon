\documentclass{article}

\usepackage{authblk}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}     % Needed before LaTeX 2018.
\usepackage{lmodern}            % To get bold-teletype.


\title{
  FELICS-AE: a framework to benchmark lightweight authenticated block ciphers
}

\author[*]{Kévin Le Gouguec}

\affil[*]{
  Airbus CyberSecurity -
  ZA Clef Saint-Pierre,
  1 Bd Jean Moulin,
  CS 40001,
  MetaPole,
  78996 ÉLANCOURT Cedex -
  France -
  \href{mailto:kevin.legouguec@airbus.com}{kevin.legouguec@airbus.com}
}


\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

The CAESAR competition~\cite{CAESAR:submissions} and the NIST
Lightweight Cryptography Standardization Process~\cite{NIST:LWC} have
brought to light several new Authenticated Encryption with Associated
Data (AEAD) schemes dedicated to ``lightweight'' use-cases.  In these
use-cases, target devices are strongly constrained in terms of
computing resources: they have limited volatile memory (RAM) and
non-volatile memory (ROM), their processors operate at low frequencies
and feature few registers, they may only be able to draw power from a
battery that can neither be recharged nor replaced, etc.

These devices thus have very few resources to spare on security.  This
implies that the aforementioned algorithms must be selected not only
for their robustness, but also according to their efficiency.  Given
two encryption schemes with equivalent security, the scheme which
leaves the target device more resources to perform its designed
function should be preferred.

Thus measuring the performance of these algorithms is an integral part
of the selection process carried out in~\cite{NIST:LWC}.  In this
paper, we present FELICS-AE, an adaptation of the FELICS
framework~\cite{FELICS:paper} dedicated to authenticated encryption
schemes, which we use to assess the performance of our candidate
\textsc{Lilliput-AE}~\cite{NIST:Lilliput-AE}.

First, in section~\ref{sec:felics}, we will present the original
FELICS framework.  We will then present FELICS-AE in
section~\ref{sec:felics-ae}, going over our work to adapt the
framework and explaining how to set it up, measure algorithm
performance, and add new encryption schemes.  We will present the
results we obtained so far in section~\ref{sec:results}.  To conclude,
we will mention possible improvements for FELICS-AE in
section~\ref{sec:future}.

\section{Background: the FELICS framework}
\label{sec:felics}

The FELICS framework\cite{FELICS:paper} includes a collection of
implementations of encryption algorithms in C, as well as a set of
shell scripts which measure the performance of these algorithms on
various microcontrollers representative of ``Internet of Things''
(IoT) situations.

\subsection{Supported devices}
\label{sec:felics/devices}

FELICS supports the following microcontrollers (\textbf{bolded} words
denote the codenames used within the framework):

\begin{itemize}
\item 8-bit \textbf{AVR} ATmega128,
\item 16-bit \textbf{MSP}430F1611,
\item 32-bit \textbf{ARM} Cortex-M3.
\end{itemize}

The AVR and MSP430 platforms are entirely simulated, which allows one
to measure algorithm performance on these microcontrollers without
physically owning them.  To measure performance on ARM, however,
FELICS requires an Arduino Due board, as well as a J-Link probe.
Algorithms can also be benchmarked on the implementer's x86 platform,
codenamed \textbf{PC}.

\subsection{Metrics}
\label{sec:felics/metrics}

\paragraph{Code size:} FELICS adds up the \texttt{text} and
\texttt{data} sections of an implementation's compiled object code, as
reported by the GNU \texttt{size} program, to measure how much space
the algorithm's footprint on non-volatile memory.

\paragraph{RAM:} to measure the working memory needed by an algorithm,
the framework runs the implementation through a debugger, spraying a
known pattern on the stack before execution and counting how many
bytes were modified after execution.  This figure is added to the
object code's \texttt{data} section.

\paragraph{Execution time:} for simulated devices, FELICS relies on
the simulator to keep track of the number of clock cycles spent on
encryption.  For other devices, FELICS uses specialized assembly
instructions to get this information.

\subsection{Distribution}
\label{sec:felics/dist}

The CryptoLUX wiki\cite{FELICS:wiki} hosts an archive containing the
FELICS's source code (algorithm implementations and benchmarking
scripts).  The wiki also provides detailed instructions to install the
dependencies FELICS needs to compile implementations and measure their
performance on every platform.

The wiki also hosts a virtual machine (32-bit Ubuntu 14.04) where all
dependencies are pre-installed.  This makes it easier to use the
framework since one then does not need to track down all of its
dependencies.

\subsection{Algorithm instrumentation}
\label{sec:felics/adding-algos}

Algorithm implementations must comply with a number of requirements in
order to work with FELICS.  This section presents some of these
constraints.

\paragraph{Algorithm entry points must conform to a specific API.}
FELICS features multiple \emph{scenarios}, which are implemented as C
files which define the \texttt{main} function that will call the
cipher implementation.  Each scenario calls the cipher with different
parameter sizes, which allows observing the evolution of the
algorithm's performance as its input grows.

Each \texttt{main} function is generic with respect to the algorithm
under test: it is expected that each implementation defines high-level
encryption and decryption functions with specific signatures, so that
scenarios can be applied to all algorithms included in FELICS.

\paragraph{Encryption and decryption code must be split across
  distinct files.}  When measuring an algorithm's code size, FELICS
outputs three distinct tallies: encryption code size, decryption code
size, and total code size.  To achieve this, FELICS requires
integrators to fill in metadata files, spelling out which object files
are used for encryption, and which are used for decryption.

This means that if an implementation originally had one file featuring
both encryption and decryption functions, an integrator must split it
into two files and tell FELICS which file serves which purpose.  If
the original file contained code used by both encryption and
decryption functions, the integrator must further create a third file
to move the common code to.

\paragraph{Array declarations must be annotated.}  FELICS defines a
set of macros that annotate integer types for two purposes:

\begin{itemize}
\item They specify optimal memory alignment for ingeter arrays arrays:
  this ensures that implementations aliasing byte arrays as
  e.g. 32-bit integer arrays do not accidentally access an array
  member at an address which is not aligned for a 32-bit variable,
  which can degrade performance or cause undefined behaviour.

\item They add the platform-dependent keywords necessary to tell the
  compiler whether arrays should be stored in ROM or RAM: e.g. for AVR
  \texttt{ROM\_DATA\_BYTE} expands to \texttt{const uint8\_t PROGMEM
    aligned}, where \texttt{PROGMEM} instructs \texttt{gcc} to move
  the variable to Program Memory; \texttt{READ\_ROM\_DATA\_BYTE}
  expands to \texttt{pgm\_read\_byte}, which performs the operations
  needed to read from this specific memory region.
\end{itemize}

An integrator must therefore go over each array declaration in an
implementation and change its type to the correct macro.

\section{FELICS for Authenticated Encryption}
\label{sec:felics-ae}

FELICS was initially developed to measure the performance of block
ciphers and stream ciphers.  In this section, we describe the changes
we made to adapt the framework to AEAD algorithms; we then describe
how it can be used and extended.

\subsection{Changes from FELICS}
\label{sec:felics-ae/diff-felics}

We began our work with release 1.1.10 of the FELICS framework.  Our
goals were to support AEAD algorithms, simplify the algorithm
integration process, and improve the feedback given to implementers
while they optimize their code.

\subsubsection{AEAD support}
\label{sec:felics-ae/aead-support}

Authenticated encryption primitives have specific signatures: their
inputs include the associated data to authenticate, a nonce to achieve
semantic security\cite{Rogaway:AEAD}; encryption produces an
authentication tag which is consumed by decryption.

The common C API we chose for cipher implementations is inspired by
the API described in the call for submissions of the CAESAR
competition\cite{CAESAR:call}, which has been re-used in the NIST
standardization process\cite{NIST:LWC-requirements}.  Our API differs
in minor ways:

\begin{itemize}
\item It does not include the \texttt{nsec} parameter, which is unused
  in the context of the NIST standardization process.  Removing this
  unused variable reduces the number of compiler warnings, which helps
  implementers spot actual errors in their code.
\item Arrays are passed as pointers to \texttt{uint8\_t} rather
  \texttt{unsigned char}.  Array sizes are passed as \texttt{size\_t}
  rather than \texttt{unsigned long long}.  The latter is
  unnecessarily large on some platforms: e.g. on AVR, an
  \texttt{unsigned long long} variable takes 8 bytes, while
  \texttt{size\_t} is only 2 bytes.
\end{itemize}

\subsubsection{Tools for implementation optimization}
\label{sec:felics-ae/tools-opt}

The original FELICS framework offers multiple entry points:

\begin{itemize}
\item one makefile for each implementation, which can compile a
  scenario for any platform, or run a scenario on the development PC,
\item one script for every metric, which run the relevant tools
  (e.g. simulators, debbugers) for a given device and algorithm, and
  either display the measurements in a human-readable table or
  serialize them in an unspecified format,
\item \texttt{collect\_ciphers\_metrics.sh}, a more complex script
  which iterates over algorithms, architectures, platforms, scenarios,
  and compiler options, and calls on the aforementioned scripts and
  makefiles to run a comprehensive measurement campaign.
\end{itemize}

While developing optimized implementations of \textsc{Lilliput-AE}, we
found that our main tasks were:

\begin{itemize}
\item running a full measurement campaign for a set of algorithms,
\item comparing a set of results against a previous set,
\item comparing two implementations of the same algorithm
  (e.g. reference vs. threshold, reference vs. optimized for a
  specific architecture),
\item exporting a set or a subset of results into various formats.
\end{itemize}

We developed a new set of scripts to perform these tasks, which we
present in section~\ref{sec:felics-ae/usage}.  We chose to implement
these tools in Python, which we found more convenient than Bash for
multiple reasons: e.g. a rich library ecosystem, reduced boilerplate
(the \texttt{argparse} library, for example, produces detailed usage
messages automatically, whereas the usage messages for the original
shell scripts are maintained manually, and may become outdated).

While adapting FELICS to AEAD algorithms and integrating
\textsc{Lilliput-AE}, \textsc{Ascon} and ACORN, we also stumbled on
several issues related to the poor error-reporting capabilities of
shell scripts:

\begin{itemize}
\item None of the framework's scripts uses the \texttt{errexit} shell
  option: command errors are only detected when they are followed by
  ad-hoc checks.
\item These checks do not necessarily stop the script, which means
  that some failures cannot be detected unless the user either
  carefully watches the framework's output, or surveys the results
  closely enough to notice suspicious patterns (e.g. metrics set to
  zero).
\item Simply setting the \texttt{errexit} option only marginally
  improves the situation, since it does not produce a backtrace when
  an error happens: tracking down a failed command which produces no
  output involves a certain amount of manual work.  The situation is
  aggravated by the framework's error-checking convention, where
  commands writing to the standard error stream are assumed to have
  failed: naively setting the \texttt{xtrace} shell option in
  sub-scripts then causes spurious failures in the parent scripts.
\end{itemize}

The Python language includes exceptions, which makes error-handling
more ergonomic and robust: failures interrupt the program flow
immediately, include precise backtraces, and may be enriched with
arbitrary information by the developer.

\subsubsection{Distribution}
\label{sec:felics-ae/distrib}

In order to make it easier to setup FELICS-AE, we wrote scripts to
fetch and install the framework's dependencies.  We further wrote
scripts to produce a ready-to-use Docker image bundling FELICS-AE with
its dependencies.

\subsubsection{Miscellaneous}
\label{sec:felics-ae/misc}

To add support for AEAD schemes, we made several changes that aimed at
simplifying the framework, removing degrees of freedom which we did
not need.  For example, we removed the ``scenario'' parameter;
measurement campaigns now always begin by checking the implementation
against a test vector, then measure the performance of encrypting 16
bytes of plaintext with 16 bytes of associated data.

Beyond encryption and decryption, FELICS further distinguished metrics
for key schedule (for block ciphers) and setup phase (for stream
ciphers).  In some AEAD algorithms (e.g. \textsc{Lilliput-AE}), the
underlying block cipher and its key schedule are called repeatedly for
every block of input; there was no obvious way to adapt the framework
to preserve this distinction, so FELICS-AE only provides metrics for
the whole encryption process.

While adapting the cycle-counting assembly code for x86\_64
architectures, we observed considerable variance in our figures for
execution time.  A lot of CPU features of modern workstations
contribute to this variance: to mitigate these factors, we implemented
several countermeasures, following Intel's guidelines for benchmarking
on IA-46 architectures\cite{Intel:64bit-ISA-bench}:

\begin{itemize}
\item Use the \texttt{cpuid} instruction to ensure that all
  instructions are serialized correctly, otherwise out-of-order
  execution may move part of the code we want to benchmark out of the
  scope of the timestamp-measuring instructions, or even move
  unrelated code inside this scope.

\item Use the \texttt{taskset} command to pin the benchmark program to
  a single core, otherwise the code may be moved to other cores while
  it runs, and their timestamp counters may not be synchronized.

\item If the user is privileged enough, set the CPU frequency scaling
  governor for this core to ``performance'' to ensure a fixed
  frequency,

\item Run the scenario multiple times, and take the median cycle count.
\end{itemize}

Though these countermeasures did reduce the variance, they did not
completely eliminate it, and we still cannot get completely
reproducible cycle counts.

\subsection{Usage}
\label{sec:felics-ae/usage}

TODO: \texttt{felics-*} suite
TODO: JSON

\section{Results}
\label{sec:results}

\section{Future work}
\label{sec:future}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
